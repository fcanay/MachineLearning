{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Informe TP 1 Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para los dos primeros experimentamos decidimos usar el dataset Pima Indians Diabetes Data Set.\n",
      "Este es un dataset de clasificaci\u00f3n num\u00e9rico, con 768 instancias y 8 atributos \n",
      "https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Objetivos"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los objetivos de este TP son analizar los clasificadores de \u00c1rboles de Decisi\u00f3n, KNN y Naive Bayes. Viendo como se comportan ellos en condiciones normales y con datos perturbados. Para entre otras cosas analizar su robustez."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IDT"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para intentar ver el efecto del overfitting, creamos un Desicion Tree variando la cantidad de nodos m\u00e1ximos a la hora de construir el \u00e1rbol"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_nodes = range(2,150,1)\n",
      "for i in max_nodes:\n",
      "    clf = tree.DecisionTreeClassifier(max_leaf_nodes=i).fit(X_train, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/IDTwithSeed60.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En el gr\u00e1fico se puede ver claramente el efecto de overfitting. El train y test score aumentan hasta alrededor de los 20 nodos m\u00e1ximos, a partir de ah\u00ed  el train score sigue subiendo hasta alcanzar el 100% de aciertos, mientras el test score empieza a caer. \n",
      "\n",
      "Esto sucede ya que el modelo generado al entrenar deja de poder generalizar y se termina transformando casi en un if gigante con todos los casos de train.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n quisimos ver c\u00f3mo afecta a los Decision Tree el agregar ruido a las clases de las mediciones.\n",
      "\n",
      "Para esto cambiamos aleatoriamente un porcentaje de los datos de train a la clase opuesta (nuestro dataset consta de dos clases).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noise_percentages = [0,0.1,0.2,0.3,0.4,0.5]\n",
      "noise_idx = np.random.random(Y_train.shape)\n",
      "    for noise in noise_percentages:\n",
      "        y_train_with_noise = Y_train.copy()\n",
      "        y_train_with_noise[noise_idx<noise] = np.ones(y_train_with_noise[noise_idx<noise].shape) - y_train_with_noise[noise_idx<noise]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/IDTwithSeed60noise50.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analizando el gr\u00e1fico anterior, no logramos ver un efecto significativo al agregar ruido a los datos.\n",
      "Por lo que concluimos que los Decision Tree son un clasificador robusto"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "KNN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n analizaremos el comportamiento de KNN, para eso previamente normalizamos los datos. Esto genera que una variable cuyo rango sea mucho mayor tenga m\u00e1s preponderancia a la hora de clasificar"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.2 ,random_state=seed)\n",
      "testScores[weights] = []\n",
      "NeighborsCount = range(1,600,20)\n",
      "    for i in NeighborsCount:\n",
      "        clf = KNeighborsClassifier(n_neighbors=i, algorithm='ball_tree',weights='distance').fit(X_train,Y_train)\n",
      "        trainScores[weights].append(clf.score(X_train,Y_train))\n",
      "        testScores[weights].append(clf.score(X_test,Y_test))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/KnnwithSeed0andWeightdistance.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En el gr\u00e1fico podemos ver otra vez el efecto de overfitting en la curva de Test score.\n",
      "\n",
      "La curva de Test converge al porcentaje de la clase mayoritaria. Ya que al tener en cuenta a todos los vecinos, el clasificador devuelve siempre la clase mayoritaria.\n",
      "\n",
      "Adem\u00e1s vemos que la curva de Train score es constantemente 1, esto ocurre ya que la funci\u00f3n de peso sobre los vecinos es 1/distancia y como el clasificador ya vio la instancia de train, la distancia a esta es 0 y su peso se vuelve infinito. Por lo que siempre acierta la clase para el conjunto de Train"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/KnnwithSeed0andWeightuniform.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En este gr\u00e1fico vemos el mismo experimento que el anterior pero usando como funci\u00f3n de peso para los vecinos 'uniform'. Que los pesa uniformemente.\n",
      "\n",
      "Otra vez se ve el overfitting, y que al crecer la cantidad de vecinos que se tienen en cuenta, el clasificador converge al porcentaje de la clase mayoritaria.\n",
      "\n",
      "Tanto Test y Train convergen a un valor cercano, ya que la muestra est\u00e1 tomada de la misma distribuci\u00f3n."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para probar el fen\u00f3meno conocido como curse of dimensionality, agregamos variables con valores al azar para probar c\u00f3mo se comportaba nuestro estimador. \n",
      "\n",
      "Un estimador robusto no deber\u00eda ver afectada su performance ya que la cantidad de informaci\u00f3n que se le provee es la misma, solo que ahora tiene que saber reconocerla entre las variables basura."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/KnnwithSeed0andWeightNoise.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podemos ver la ca\u00edda significativa en la performance entre ambas. \n",
      "\n",
      "Esto se debe a que el KNN toma los vecinos cercanos para clasificar. Al agregar variables aleatorias los puntos se expanden en m\u00e1s dimensiones y cada vez pesan menos las variables que tienen informaci\u00f3n relevantes."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n usaremos Naive Bayes para analizar noticias de diario para inferir a que secci\u00f3n pertenecen\n",
      "\n",
      "Antes de esto es necesario preprocesar el texto para que el clasificador pueda entenderlo\n",
      "\n",
      "Para esto usamos el HashingVector provisto por sklearn para preprocesar el texto de las noticias y sacar las stopwords\n",
      "\n",
      "Tomamos como stop words para el idioma castellano de http://www.ranks.nl/stopwords/spanish"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop_words = ['un','una','unas','unos','uno','sobre','todo','tambi\u00e9n','tras','otro','alg\u00fan','alguno','alguna','algunos','algunas','ser','es','soy','eres','somos','sois','estoy','esta','estamos','estais','estan','como','en','para','atras','porque','porqu\u00e9','estado','estaba','ante','antes','siendo','ambos','pero','por','poder','puede','puedo','podemos','podeis','pueden','fui','fue','fuimos','fueron','hacer','hago','hace','hacemos','haceis','hacen','cada','fin','incluso','primero','desde','conseguir','consigo','consigue','consigues','conseguimos','consiguen','ir','voy','va','vamos','vais','van','vaya','gueno','ha','tener','tengo','tiene','tenemos','teneis','tienen','el','la','lo','las','los','su','aqui','mio','tuyo','ellos','ellas','nos','nosotros','vosotros','vosotras','si','dentro','solo','solamente','saber','sabes','sabe','sabemos','sabeis','saben','ultimo','largo','bastante','haces','muchos','aquellos','aquellas','sus','entonces','tiempo','verdad','verdadero','verdadera','cierto','ciertos','cierta','ciertas','intentar','intento','intenta','intentas','intentamos','intentais','intentan','dos','bajo','arriba','encima','usar','uso','usas','usa','usamos','usais','usan','emplear','empleo','empleas','emplean','ampleamos','empleais','valor','muy','era','eras','eramos','eran','modo','bien','cual','cuando','donde','mientras','quien','con','entre','sin','trabajo','trabajar','trabajas','trabaja','trabajamos','trabajais','trabajan','podria','podrias','podriamos','podrian','podriais','yo','aquel']\n",
      "\n",
      "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
      "                               non_negative=True,stop_words=stop_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Luego dividimos en un conjunto de train y otro de test, creamos el clasificador, lo entrenemos, testeamos y conseguimos la matriz de confusi\u00f3n "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
      "clf = MultinomialNB(alpha=0.01)\n",
      "clf.fit(X_train,Y_train)\n",
      "conf_matrix = metrics.confusion_matrix(Y_test,clf.predict(X_test)) \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/conf_matrix_0.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podemos ver la matriz de confusi\u00f3n generada por el clasificador. Las diagonales son los aciertos, mientras que el resto de la matriz son los errores. Como vemos en la mayor\u00eda de los casos el estimador logr\u00f3 acertar la clase correspondiente"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/conf_matrix_0_normalizada.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Al normalizar la matriz anterior podemos ver que las clases del centro de la matriz aumentaron su color mostraron que tienen un alto porcentaje de aciertos.\n",
      "\n",
      "Pero tambi\u00e9n se logra ver que las clases Cultura y Tecnolog\u00eda no tienen un tan alto desempe\u00f1o. Como hab\u00eda pocos casos de estas clases, en la matriz sin normalizar no se notaban esto."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusi\u00f3n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como conclusi\u00f3n de este trabajo logramos ver que:\n",
      "\n",
      "\n",
      "$\\bullet$ Tanto en \u00c1rboles de Decisi\u00f3n como en KNN encontramos el problema del overfitting\n",
      "\n",
      "$\\bullet$ Los \u00c1rboles de Decisi\u00f3n son robustos ante el ruido en los datos\n",
      "\n",
      "$\\bullet$ En cambio KNN es sensible a variables ruidosas que no aportan informaci\u00f3n, sufriendo la curse of dimensionality\n",
      "\n",
      "$\\bullet$ Es importante normalizar los datos antes de usar un KNN, sino las variables de mayor rango tendran m\u00e1s importancia\n",
      "\n",
      "$\\bullet$ Al hacer an\u00e1lisis de texto es necesario preprocesarlo, quitando las stop words y vectorizandolo"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}